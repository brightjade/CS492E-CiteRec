# Final Report

## Quality Arguments

> 

## Evaluation

> We evaluated ICARUS by running a user study with five graduate students in a lab (4 Master’s and 1 Ph.D candidate) who have experience of writing research paper for at least one year. After giving a short instruction of how ICARUS works, we asked them to explore ICARUS freely and give feedback and general impressions about it. For the post survey, we asked three questions: Q1) Illustrate general impression, pros/cons, feed backs, further improvements of ICARUS, Q2) What kind of recommendation did you feel helpful?, and Q3) If ICARUS is plugged in overleaf or any writing tool you are using, would you use it? It took an average of thirty minutes for running a user study.

> For evaluation, we first conducted NASA-TLX which finds the overall demand when using an interface. Although there were five participants, we tried to find out if ICARUS imposes high mental demand or frustration level. It turned out that the average scores of mental demand and the frustration level were under 2, indicating that ICARUS is fairly easy and straightforward to use. 

> We report the pros and cons that participants pointed out in the post survey as the qualitative analysis. For the pros, all participants replied that ICARUS is easy to use and has a straightforward design. One participant replied that he liked how ICARUS integrated the 2D visualization view (P2). Also, P4 noted that refined search was useful and novel. On the other side of the coin, two participants (P1, P5) pointed out the failure cases of recommendations were somehow disappointing. “At least, I expect that the titles include the query” (P1). Additionally, three participants (P2, P3, P4) stressed that the recommendation view should have included the metadata such as conference, title, and year. Lastly, four participants replied that they will use ICARUS if it is plugged in the text editor they are using such as Overleaf. 

> We also received further suggestions to improve ICARUS. P1 noted that the connectivity between papers is important when finding the paper. “Citation information between papers should be considered to express the connectivity between the papers.” Another fruitful suggestion from P1 was that weighting the attention of words could improve the recommendation quality. “Giving more weights to the query keywords (e.g. interactive segmentation) is needed to find more relevant papers.” This is a reasonable feedback since most of the state-of-the-art language models utilize attention mechanisms in their model. We believe including such feature would enhance the recommendation quality of ICARUS. 

> What we learned from evaluating ICARUS is that users actively used 2D projection view when exploring papers. Also, users considered the metadata such as conference or year important when choosing the recommendation. Additionally, we were delighted that users said that the refining feature actually improved the quality of the recommendation. Refining the query via human interaction is the collaboration of the language model and the user’s taste. The fact that users liked this feature demonstrates that we built a system which actively utilizes the collaboration of human and AI. 

## Discussion

> We designed ICARUS such that it supports a number of human-AI interactions in an effective and intuitive manner; however, we believe that it can be much improved to make the user experience better. First, ICARUS is an AI-advised decision-making system where the user has a flexible control over AI. The user can refine the results by adding and blacklisting recommended papers, as well as performing the “Refined Search” feature. Nevertheless, we realize that grasping the mental model of our system is difficult because random papers are very often recommended. As for the interpretability of the model, we had thought that displaying the cosine similarity score could explain the model decisions, but it lacked demonstrating the process of the model making decisions. To enhance the interpretability, we could perhaps highlight specific parts of the paper abstract that contributed the most on the similarity score. Alternatively, we could create a more complex yet interpretable scoring function than cosine similarity. A cosine similarity score ranks papers based on semantic similarity, but this might not be what the user actually wants. We could utilize additional data, such as the number of connected citations, user preferences, or even a past session history of the user, to build a more comprehensive and explainable recommendation engine.

> In terms of metrics, we had no methods of evaluating the model performance quantitatively. Paper recommendation does not really have a clear answer, as the standards for “good” recommendation differ subjectively. We could have done an A/B testing over users to build an optimized recommendation algorithm, but the amount of time and resources would be beyond the scope of a class project. In such a case, automatic evaluation metrics such as Mean Average Precision at K (MAP@K), one of the metrics for evaluating recommendation systems, could be considered.

> As for the fairness, ethics, and privacy issues, we believe that ICARUS does not cause any serious problem. It is perfectly fair in that the papers are recommended solely based on abstracts and not biased in any way, since author names, university/corporate names, or any other metadata that could hint at the identities of the papers were not given at all. However, we cannot be sure of this, as we had not run any test to check this. Moreover, such a perfectly unbiased decision-making system might not be wanted by the users because according to one of the comments from the user study, users might actually want to see popular papers or papers from well-known authors/companies first. We could perhaps add a filtering feature in which the users could select.

> Lastly, we believe that the 2-D scatterplot was simple yet clear enough to visualize well of the embedding space of the papers. But as opposed to our initial belief, the cartesian distance did not reflect the actual similarity score. Therefore, instead of visualizing the distance in the cartesian plane, we could increase the size of scatter dots by their similarity score and create an interactive graph with the dots.


## Individual Reflection

### Seokhun Jeong

>

### Jungsoo Lee

> I was lucky to work with my wonderful teammates who had strong coding/communication skills. Minseok was at the core of our work who integrated the front-end and back-end of ICARUS. Seokhun had strong coding skills at building the front-end of ICARUS and gave various suggestions for the views in ICARUS. I deployed the pretrained language models in order to extract the features from abstracts and used them for 2D visualization. I really liked how my teammates compensated the parts that I could think while we had discussions. For example, I tend to think about the model implementation part when conceiving a new feature. However, Seokhun and Minseok point out the impacts on the front-end side which I could never think of. Also, we all had different abilities, so we compensated each other in the coding part too. One thing that could have been better is that we only discussed online which somehow limited our discussion. Though, it was a great experience to work with the two members.

> Through the course, I learned that human interaction is more complicated than I thought. Since I have a strong background at AI, I normally think about the test accuracy or any kind of performance that we can measure for a model. However, during the course, I had to think how humans react to a certain feature which was far more complicated than I expected and it opened my perspectives. I will take this as the cornerstone and think about various aspects when building a model including the human’s perspective.

### Minseok Choi

> Personally, I had one of the best team experiences with our team because everyone was good at different things yet shared the same picture of the final product. Seokhun was an expert at handling the frontend with React, bootstrapping with Nextjs and managing component states with MobX (which I completely had no idea and learned another level of React programming). Jungsoo had an experience with building a platform in the HCI field, so he knew well of the user needs for the backend AI model and understood how the user studies and interviews proceed. Finally, I had an experience with deploying a React-Flask application from scratch, so it was very comfortable combining the best of the two teammates. One thing we could have done better, however, is planning the AI design process more effectively. Although the project put a heavy emphasis on the human-AI interactions, we failed to think through how much time and effort would be required for implementing the interactions. Therefore, most of the workload was focused on the frontend before each deadline, so I, who knew React, communicated with Seokhun consistently and helped implementing React components. Moreover, Jungsoo finished building our AI model early, so he helped making slides and report, as well as conducting the user studies. Due to everyone’s efforts, we were able to pull through, but for the next time, we would need a much more careful planning before designing our human-AI interactive application.

> Through designing a human-AI application as a team, I learned to think in a user-centered perspective. Before implementing each component or interaction, I would think twice whether the user would find it convenient or unintuitive. Furthermore, I did not consider the explainability or interpretability of the AI model as important, but putting myself in their shoes, I realized that the individual AI performance is not everything and that demonstrating the reason for the AI decisions is crucial. It would not only help building the mental model of AI, but it would also assist in the human-AI team collaboration performance. Lastly, by developing a web-based platform, I felt firsthand that the fast inference speed is really important for good user experience. I learned to appreciate lightweight models, especially in class projects, and would think more about the computation speed and resources in my future research and beyond.
